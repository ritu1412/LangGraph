{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chatbot with Langgraph with LLM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"CourseLanggraph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for using GROQ: Fast AI inference for openly-available models (Low latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1222af9a0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1222bc6d0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "  # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "  messages:Annotated[list,add_messages]\n",
    "\n",
    "graph_builder=StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1222bcbe0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state:State):\n",
    "  return {\"messages\":llm.invoke(state['messages'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1222bcbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"chatbot\",chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1222bcbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1222bcbe0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(START,\"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([{'messages': AIMessage(content='Hello! 👋 How can I help you today? 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 10, 'total_tokens': 24, 'completion_time': 0.025454545, 'prompt_time': 4.3e-07, 'queue_time': 0.014505758, 'total_time': 0.025454975}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a77921ea-db33-43a7-bba5-4f22fd76c2b2-0', usage_metadata={'input_tokens': 10, 'output_tokens': 14, 'total_tokens': 24})}])\n",
      "content='Hello! 👋 How can I help you today? 😊\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 10, 'total_tokens': 24, 'completion_time': 0.025454545, 'prompt_time': 4.3e-07, 'queue_time': 0.014505758, 'total_time': 0.025454975}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-a77921ea-db33-43a7-bba5-4f22fd76c2b2-0' usage_metadata={'input_tokens': 10, 'output_tokens': 14, 'total_tokens': 24}\n",
      "Assistant: Hello! 👋 How can I help you today? 😊\n",
      "\n",
      "dict_values([{'messages': AIMessage(content='## Understanding Self-Attention: A Spotlight on Importance\\n\\nImagine you\\'re reading a sentence: \"The cat sat on the mat.\"\\n\\nYour brain doesn\\'t process each word in isolation. You understand the meaning by paying attention to the relationships between words. For example, you know \"cat\" and \"sat\" are related, and \"mat\" is where the cat sat.\\n\\nSelf-attention allows AI models to do the same thing. It\\'s a mechanism that lets a model weigh the importance of different words in a sequence when processing information. \\n\\n**Here\\'s a breakdown:**\\n\\n1. **Query, Key, Value:** Each word in a sequence is represented as three vectors:\\n    - **Query:** Represents what the word is looking for.\\n    - **Key:** Represents what the word offers.\\n    - **Value:** Contains the actual information of the word.\\n\\n2. **Calculating Attention Scores:**\\n\\n   - The query of each word is compared to the keys of all other words in the sequence. This comparison generates attention scores, indicating how relevant each word is to the current word.\\n\\n3. **Weighted Sum:**\\n\\n   - The attention scores are normalized (converted to probabilities) and used to weight the values of all words. This creates a weighted sum, emphasizing the information from the most relevant words.\\n\\n4. **Contextualized Representation:**\\n\\n   - The weighted sum becomes the new representation of the word, enriched with context from related words.\\n\\n**Analogy:** Think of it like shining a spotlight on different parts of a sentence. The spotlight (attention) focuses on the most important words, highlighting their relevance to the current word.\\n\\n**Benefits of Self-Attention:**\\n\\n- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can consider relationships between words that are far apart in a sequence.\\n- **Parallelizable:** Attention calculations can be performed in parallel, making training faster.\\n- **Interpretability:** Attention scores provide insights into which words are important for understanding a given context.\\n\\n**Applications:**\\n\\nSelf-attention is a key ingredient in many powerful AI models, including:\\n\\n- **Transformers:** The foundation of models like BERT, GPT-3, and T5, revolutionizing natural language processing.\\n- **Image Recognition:** Used in Vision Transformers (ViT) to analyze images by attending to different regions.\\n- **Speech Recognition:** Enhances the understanding of spoken language by focusing on relevant phonemes.\\n\\n\\n\\nLet me know if you\\'d like to dive deeper into a specific aspect of self-attention!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 535, 'prompt_tokens': 14, 'total_tokens': 549, 'completion_time': 0.972727273, 'prompt_time': 8.2279e-05, 'queue_time': 0.014468349, 'total_time': 0.972809552}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8fb2dd8-b68f-4e9b-b7a8-9141f5b68456-0', usage_metadata={'input_tokens': 14, 'output_tokens': 535, 'total_tokens': 549})}])\n",
      "content='## Understanding Self-Attention: A Spotlight on Importance\\n\\nImagine you\\'re reading a sentence: \"The cat sat on the mat.\"\\n\\nYour brain doesn\\'t process each word in isolation. You understand the meaning by paying attention to the relationships between words. For example, you know \"cat\" and \"sat\" are related, and \"mat\" is where the cat sat.\\n\\nSelf-attention allows AI models to do the same thing. It\\'s a mechanism that lets a model weigh the importance of different words in a sequence when processing information. \\n\\n**Here\\'s a breakdown:**\\n\\n1. **Query, Key, Value:** Each word in a sequence is represented as three vectors:\\n    - **Query:** Represents what the word is looking for.\\n    - **Key:** Represents what the word offers.\\n    - **Value:** Contains the actual information of the word.\\n\\n2. **Calculating Attention Scores:**\\n\\n   - The query of each word is compared to the keys of all other words in the sequence. This comparison generates attention scores, indicating how relevant each word is to the current word.\\n\\n3. **Weighted Sum:**\\n\\n   - The attention scores are normalized (converted to probabilities) and used to weight the values of all words. This creates a weighted sum, emphasizing the information from the most relevant words.\\n\\n4. **Contextualized Representation:**\\n\\n   - The weighted sum becomes the new representation of the word, enriched with context from related words.\\n\\n**Analogy:** Think of it like shining a spotlight on different parts of a sentence. The spotlight (attention) focuses on the most important words, highlighting their relevance to the current word.\\n\\n**Benefits of Self-Attention:**\\n\\n- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can consider relationships between words that are far apart in a sequence.\\n- **Parallelizable:** Attention calculations can be performed in parallel, making training faster.\\n- **Interpretability:** Attention scores provide insights into which words are important for understanding a given context.\\n\\n**Applications:**\\n\\nSelf-attention is a key ingredient in many powerful AI models, including:\\n\\n- **Transformers:** The foundation of models like BERT, GPT-3, and T5, revolutionizing natural language processing.\\n- **Image Recognition:** Used in Vision Transformers (ViT) to analyze images by attending to different regions.\\n- **Speech Recognition:** Enhances the understanding of spoken language by focusing on relevant phonemes.\\n\\n\\n\\nLet me know if you\\'d like to dive deeper into a specific aspect of self-attention!\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 535, 'prompt_tokens': 14, 'total_tokens': 549, 'completion_time': 0.972727273, 'prompt_time': 8.2279e-05, 'queue_time': 0.014468349, 'total_time': 0.972809552}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-f8fb2dd8-b68f-4e9b-b7a8-9141f5b68456-0' usage_metadata={'input_tokens': 14, 'output_tokens': 535, 'total_tokens': 549}\n",
      "Assistant: ## Understanding Self-Attention: A Spotlight on Importance\n",
      "\n",
      "Imagine you're reading a sentence: \"The cat sat on the mat.\"\n",
      "\n",
      "Your brain doesn't process each word in isolation. You understand the meaning by paying attention to the relationships between words. For example, you know \"cat\" and \"sat\" are related, and \"mat\" is where the cat sat.\n",
      "\n",
      "Self-attention allows AI models to do the same thing. It's a mechanism that lets a model weigh the importance of different words in a sequence when processing information. \n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "1. **Query, Key, Value:** Each word in a sequence is represented as three vectors:\n",
      "    - **Query:** Represents what the word is looking for.\n",
      "    - **Key:** Represents what the word offers.\n",
      "    - **Value:** Contains the actual information of the word.\n",
      "\n",
      "2. **Calculating Attention Scores:**\n",
      "\n",
      "   - The query of each word is compared to the keys of all other words in the sequence. This comparison generates attention scores, indicating how relevant each word is to the current word.\n",
      "\n",
      "3. **Weighted Sum:**\n",
      "\n",
      "   - The attention scores are normalized (converted to probabilities) and used to weight the values of all words. This creates a weighted sum, emphasizing the information from the most relevant words.\n",
      "\n",
      "4. **Contextualized Representation:**\n",
      "\n",
      "   - The weighted sum becomes the new representation of the word, enriched with context from related words.\n",
      "\n",
      "**Analogy:** Think of it like shining a spotlight on different parts of a sentence. The spotlight (attention) focuses on the most important words, highlighting their relevance to the current word.\n",
      "\n",
      "**Benefits of Self-Attention:**\n",
      "\n",
      "- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can consider relationships between words that are far apart in a sequence.\n",
      "- **Parallelizable:** Attention calculations can be performed in parallel, making training faster.\n",
      "- **Interpretability:** Attention scores provide insights into which words are important for understanding a given context.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Self-attention is a key ingredient in many powerful AI models, including:\n",
      "\n",
      "- **Transformers:** The foundation of models like BERT, GPT-3, and T5, revolutionizing natural language processing.\n",
      "- **Image Recognition:** Used in Vision Transformers (ViT) to analyze images by attending to different regions.\n",
      "- **Speech Recognition:** Enhances the understanding of spoken language by focusing on relevant phonemes.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to dive deeper into a specific aspect of self-attention!\n",
      "\n",
      "dict_values([{'messages': AIMessage(content='Self-attention is a powerful mechanism in deep learning, particularly in the field of Natural Language Processing (NLP).  It allows a model to weigh the importance of different words in a sequence when processing information.\\n\\n**Imagine you\\'re reading a sentence:**\\n\\n\"The cat sat on the mat.\"\\n\\nSelf-attention helps the model understand that \"cat\" is related to \"sat\" and \"mat,\" but \"the\" is less directly related. It does this by calculating \"attention weights\" that indicate how much each word should influence the understanding of other words.\\n\\n**Here\\'s a breakdown:**\\n\\n1. **Query, Key, Value:** Each word in the sequence is represented as three vectors:\\n\\n   - **Query (Q):** Represents what the word is \"looking for\" in the sequence.\\n   - **Key (K):** Represents what information the word \"holds.\"\\n   - **Value (V):** Contains the actual information associated with the word.\\n\\n2. **Attention Scores:**  The model calculates a score for each pair of words based on how well their query and key vectors match. This score represents the \"attention\" paid by one word to another.\\n\\n3. **Softmax Normalization:** The attention scores are normalized using a softmax function, which transforms them into probabilities. This ensures that the weights sum up to 1, meaning each word\\'s contribution is proportionally distributed.\\n\\n4. **Weighted Sum:**  The values associated with each word are then weighted by the attention probabilities. This creates a context-aware representation of each word, taking into account its relationships with other words in the sequence.\\n\\n**Benefits of Self-Attention:**\\n\\n- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can effectively process relationships between words that are far apart in a sequence.\\n- **Parallel Processing:**  Self-attention can be computed in parallel, making it more efficient than sequential models like RNNs.\\n- **Interpretability:** The attention weights provide insights into how the model is understanding the relationships between words.\\n\\n**Applications:**\\n\\nSelf-attention is widely used in:\\n\\n- **Machine Translation:**  Understanding the context of words in a sentence to accurately translate them.\\n- **Text Summarization:**  Identifying the most important words and phrases in a document.\\n- **Question Answering:**  Focusing on relevant information in a text to answer a specific question.\\n- **Chatbots:**  Generating more natural and context-aware responses.\\n\\n\\n\\nLet me know if you\\'d like a more technical explanation or have any other questions!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 533, 'prompt_tokens': 13, 'total_tokens': 546, 'completion_time': 0.969090909, 'prompt_time': 8.806e-05, 'queue_time': 0.013351409000000002, 'total_time': 0.969178969}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0bb1ba04-9611-459d-b419-d431d13e8137-0', usage_metadata={'input_tokens': 13, 'output_tokens': 533, 'total_tokens': 546})}])\n",
      "content='Self-attention is a powerful mechanism in deep learning, particularly in the field of Natural Language Processing (NLP).  It allows a model to weigh the importance of different words in a sequence when processing information.\\n\\n**Imagine you\\'re reading a sentence:**\\n\\n\"The cat sat on the mat.\"\\n\\nSelf-attention helps the model understand that \"cat\" is related to \"sat\" and \"mat,\" but \"the\" is less directly related. It does this by calculating \"attention weights\" that indicate how much each word should influence the understanding of other words.\\n\\n**Here\\'s a breakdown:**\\n\\n1. **Query, Key, Value:** Each word in the sequence is represented as three vectors:\\n\\n   - **Query (Q):** Represents what the word is \"looking for\" in the sequence.\\n   - **Key (K):** Represents what information the word \"holds.\"\\n   - **Value (V):** Contains the actual information associated with the word.\\n\\n2. **Attention Scores:**  The model calculates a score for each pair of words based on how well their query and key vectors match. This score represents the \"attention\" paid by one word to another.\\n\\n3. **Softmax Normalization:** The attention scores are normalized using a softmax function, which transforms them into probabilities. This ensures that the weights sum up to 1, meaning each word\\'s contribution is proportionally distributed.\\n\\n4. **Weighted Sum:**  The values associated with each word are then weighted by the attention probabilities. This creates a context-aware representation of each word, taking into account its relationships with other words in the sequence.\\n\\n**Benefits of Self-Attention:**\\n\\n- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can effectively process relationships between words that are far apart in a sequence.\\n- **Parallel Processing:**  Self-attention can be computed in parallel, making it more efficient than sequential models like RNNs.\\n- **Interpretability:** The attention weights provide insights into how the model is understanding the relationships between words.\\n\\n**Applications:**\\n\\nSelf-attention is widely used in:\\n\\n- **Machine Translation:**  Understanding the context of words in a sentence to accurately translate them.\\n- **Text Summarization:**  Identifying the most important words and phrases in a document.\\n- **Question Answering:**  Focusing on relevant information in a text to answer a specific question.\\n- **Chatbots:**  Generating more natural and context-aware responses.\\n\\n\\n\\nLet me know if you\\'d like a more technical explanation or have any other questions!\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 533, 'prompt_tokens': 13, 'total_tokens': 546, 'completion_time': 0.969090909, 'prompt_time': 8.806e-05, 'queue_time': 0.013351409000000002, 'total_time': 0.969178969}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-0bb1ba04-9611-459d-b419-d431d13e8137-0' usage_metadata={'input_tokens': 13, 'output_tokens': 533, 'total_tokens': 546}\n",
      "Assistant: Self-attention is a powerful mechanism in deep learning, particularly in the field of Natural Language Processing (NLP).  It allows a model to weigh the importance of different words in a sequence when processing information.\n",
      "\n",
      "**Imagine you're reading a sentence:**\n",
      "\n",
      "\"The cat sat on the mat.\"\n",
      "\n",
      "Self-attention helps the model understand that \"cat\" is related to \"sat\" and \"mat,\" but \"the\" is less directly related. It does this by calculating \"attention weights\" that indicate how much each word should influence the understanding of other words.\n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "1. **Query, Key, Value:** Each word in the sequence is represented as three vectors:\n",
      "\n",
      "   - **Query (Q):** Represents what the word is \"looking for\" in the sequence.\n",
      "   - **Key (K):** Represents what information the word \"holds.\"\n",
      "   - **Value (V):** Contains the actual information associated with the word.\n",
      "\n",
      "2. **Attention Scores:**  The model calculates a score for each pair of words based on how well their query and key vectors match. This score represents the \"attention\" paid by one word to another.\n",
      "\n",
      "3. **Softmax Normalization:** The attention scores are normalized using a softmax function, which transforms them into probabilities. This ensures that the weights sum up to 1, meaning each word's contribution is proportionally distributed.\n",
      "\n",
      "4. **Weighted Sum:**  The values associated with each word are then weighted by the attention probabilities. This creates a context-aware representation of each word, taking into account its relationships with other words in the sequence.\n",
      "\n",
      "**Benefits of Self-Attention:**\n",
      "\n",
      "- **Captures Long-Range Dependencies:** Unlike traditional RNNs, self-attention can effectively process relationships between words that are far apart in a sequence.\n",
      "- **Parallel Processing:**  Self-attention can be computed in parallel, making it more efficient than sequential models like RNNs.\n",
      "- **Interpretability:** The attention weights provide insights into how the model is understanding the relationships between words.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Self-attention is widely used in:\n",
      "\n",
      "- **Machine Translation:**  Understanding the context of words in a sentence to accurately translate them.\n",
      "- **Text Summarization:**  Identifying the most important words and phrases in a document.\n",
      "- **Question Answering:**  Focusing on relevant information in a text to answer a specific question.\n",
      "- **Chatbots:**  Generating more natural and context-aware responses.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like a more technical explanation or have any other questions!\n",
      "\n",
      "Good Bye\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  user_input=input(\"User: \")\n",
    "  if user_input.lower() in [\"quit\",\"q\"]:\n",
    "    print(\"Good Bye\")\n",
    "    break\n",
    "  for event in graph.stream({'messages':(\"user\",user_input)}):\n",
    "    print(event.values())\n",
    "    for value in event.values():\n",
    "      print(value['messages'])\n",
    "      print(\"Assistant:\",value[\"messages\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
